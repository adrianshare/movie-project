{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Theme Extraction and Labeling\n",
    "\n",
    "The goal of this project is to automate the extraction of labeled themes from movie overviews. These themes can replace the overview as a faster and more consistent way of determining what the movie is about. The defined themes would also allow for faster filtering that would allow the user to find a more exact match based on viewing mood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Data used for this project can be found here https://www.kaggle.com/rounakbanik/the-movies-dataset\n",
    "\n",
    "We will use the overviews and titles from the movies_metadata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('the-movies-dataset\\movies_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overviews = df['overview'].apply(str).tolist()\n",
    "overviews = \" \".join(overviews)    #prepare for word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "Names like Alan and Jack are common and will appear influence topic model distributions. Given that names are interchangeable when discussing themes, we will identify and remove them as a part of the data preparation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(overviews)\n",
    "tokens_pos = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition:\n",
    "\n",
    "Use named entity recognition to detect names in the corpus. The ne_chunk function is used to extract chunks from the tagged pos tokens and will return person, places and organizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_tree = nltk.ne_chunk(tokens_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build list of people to remove\n",
    "people = []                                 \n",
    "for subtree in ne_tree.subtrees():\n",
    "    if subtree.label() == 'PERSON':\n",
    "        person = []\n",
    "        for key, value in subtree.leaves():\n",
    "            person.append(key)\n",
    "        people.append(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counter gives count of each name \n",
    "import collections, operator\n",
    "people_unique = collections.Counter(map(tuple, people))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity recognition identifies some chunks that are not names. Using the frequency list of people_unique and manual check of the high frequency terms is done to create a list of misidentified entites that should remain in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_list = [('Hollywood'),('Christmas','Eve'),('San', 'Francisco'), ('Las', 'Vegas'), \n",
    "                ('Santa', 'Claus'), ('Christmas',), ('Hollywood',), ('Superman',), ('Academy',), \n",
    "                ('Godzilla',), ('Academy', 'Award'), ('Buenos', 'Aires'), ('Jesus', 'Christ'),\n",
    "                ('Pearl', 'Harbor'), ('Louisiana',), ('Sequel',), ('Father',), ('Wealthy',), \n",
    "                ('Disney',), ('Count', 'Dracula'),('Los', 'Angeles'), ('Monster', 'High'), \n",
    "                ('Brazil',), ('Shaolin',), ('Halloween',), ('Navy','SEALS')\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return clean list of names minus the misidentified names\n",
    "people_clean = []\n",
    "for key,val in sorted(people_unique.items(), key=operator.itemgetter(1), reverse = True):\n",
    "    if key not in exclude_list:\n",
    "        people_clean.append(' '.join(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep data:\n",
    "\n",
    "* remove people's names\n",
    "* remove stop words\n",
    "* remove punctuation\n",
    "* lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "stops = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "other_stops = [\"'s\",\"--\",\"...\"]\n",
    "stops.extend(other_stops)\n",
    "\n",
    "\n",
    "texts = []\n",
    "for sent in df['overview'].apply(str):\n",
    "    text = nltk.word_tokenize(sent)\n",
    "    text = ['PERSON' if word in people_clean else word for word in text  ]  #replace name with PERSON\n",
    "    text = [word.lower() for word in text] \n",
    "    text = [word for word in text if word not in stops]\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    text = [word for word in text if word not in string.punctuation ] \n",
    "    texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional cleaning \n",
    "more_stops = [\"``\",\"'ll\",\"''\"]\n",
    "texts_clean = [[word for word in text if word != 'person'] for text in texts]     #remove identified names\n",
    "texts_clean = [[word for word in text if word not in more_stops] for text in texts_clean]\n",
    "texts_clean = [[word for word in text if len(word) > 2] for text in texts_clean]\n",
    "print(texts_clean[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA topic modeling\n",
    "\n",
    "Latent Dirichlet Allocation will be used for topic modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build bag of words representation of all documents \n",
    "dictionary = corpora.Dictionary(texts_clean)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_clean ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 300 topics chosen after multiple iterations and coherence score testing\n",
    "\n",
    "Choosing the right number of topics is an iterative process. After multiple tests, the number of topics that gave the best results was 300. The coherence score and visualization of the topic models for human readability were part of the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=300, update_every=1, chunksize=5000, passes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get coherence score \n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "cm = CoherenceModel(model=lda, texts=texts_clean, corpus=corpus, coherence='c_v')\n",
    "coherence = cm.get_coherence()  \n",
    "print(coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pyLDAvis library provides a convenient way to visualize the topics and the distribution of terms in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "vis = pyLDAvis.gensim.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed Labels\n",
    "\n",
    "Two types of chunk patterns will form the basis of the seed phrases. We are looking to form two word phrases that represent things and terms as described by the corpus. The idea is that themes would be explicitly described during some of the overview creations and that similarity mesures can be used to aplly the common themes as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing_chunker = \"CHUNK: {<NN><NN>}\"\n",
    "term_chunker = \"CHUNK: {<JJ.*><NN>}\"\n",
    "thing_parser = nltk.RegexpParser(thing_chunker)\n",
    "term_parser = nltk.RegexpParser(term_chunker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing_tree = thing_parser.parse(tokens_pos)\n",
    "term_tree = term_parser.parse(tokens_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for subtree in thing_tree.subtrees():\n",
    "    if subtree.label() == 'CHUNK':\n",
    "        label = []\n",
    "        for key, value in subtree.leaves():\n",
    "            label.append(key)\n",
    "        #print(label)\n",
    "        labels.append(label)\n",
    "for subtree in term_tree.subtrees():\n",
    "    if subtree.label() == 'CHUNK':\n",
    "        label = []\n",
    "        for key, value in subtree.leaves():\n",
    "            label.append(key)\n",
    "        #print(label)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = [list(label) for label in set(tuple(label) for label in labels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the seed list to find potential matches that contain a specific word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "potentials = [label for label in unique_labels if 'club' in label]\n",
    "potentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High potential phrases\n",
    "\n",
    "Here we will collect the top high potential phrases for each topic. \n",
    "1. For each word in the topic model find potentials that have one matching word\n",
    "2. Convert each matching phrase to a bow model using the same dictionary used for lda training\n",
    "3. Get topic model for the phrase\n",
    "4.  Save if topic matches current topic and probability is high "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_matches = {}\n",
    "for topic_id, words in lda.print_topics(300):\n",
    "    top_match = []\n",
    "    top = 0\n",
    "    for word, rat in lda.show_topic(topic_id):\n",
    "        potentials = [label for label in unique_labels if word in label]\n",
    "        for potential in potentials:\n",
    "            unseen = dictionary.doc2bow(potential)\n",
    "            for topic, prob in lda[unseen]:\n",
    "                if topic ==topic_id and prob > 0.6:\n",
    "                    if topic in top_matches:\n",
    "                        top_matches[topic].append(list(potential))\n",
    "                    else:\n",
    "                        top_matches.update({topic : [list(potential)]})\n",
    "                    if prob > top:\n",
    "                        top = prob\n",
    "                        top_match = list(potential)\n",
    "    print('Top Match:',topic_id, top_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace overview with a list of themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Movie class\n",
    "class Movie:\n",
    "    def __init__(self, doc_id, title, overview, topics, labels, label_score):\n",
    "        self.doc_id = doc_id\n",
    "        self.title = title\n",
    "        self.overview = overview\n",
    "        self.topics = topics\n",
    "        self.labels = dict(labels)\n",
    "        self.label_score = label_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf model to be used to improve similarity scores\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a single label for each topic in an overview\n",
    "\n",
    "1. Get all the topics  above a minimum threshold for each document\n",
    "2. Get all the high potential phrases for the topic\n",
    "3. Covert the phrase to a tfidf bow model\n",
    "4. Compare phrase and document cosine similarity \n",
    "5. Keep highest score above a minimum threshold\n",
    "\n",
    "If no label meets the minimum threshold then we ignore that topic for overview replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "\n",
    "\n",
    "def get_movie_labels(doc_id):  \n",
    "    top_pot_labels = {}\n",
    "    for topic_id,prob in lda.get_document_topics(corpus[doc_id], minimum_probability=0.03): \n",
    "        top_pot_score = 0\n",
    "        top_pot_label = []\n",
    "        for pot in top_matches[topic_id]:\n",
    "            pot_corpus = tfidf[dictionary.doc2bow(pot)]\n",
    "            #label score\n",
    "            pot_score = matutils.cossim(tfidf[corpus[doc_id]], pot_corpus)\n",
    "            if pot_score > top_pot_score and pot_score > 0.05:\n",
    "                top_pot_label = list(pot)\n",
    "                top_pot_score = pot_score\n",
    "        if len(top_pot_label) > 0 : top_pot_labels.update({topic_id : list(top_pot_label)})\n",
    "\n",
    "    \n",
    "    top_pot_sent = []\n",
    "    for key in top_pot_labels:\n",
    "        top_pot_sent.append(list(top_pot_labels[key]))\n",
    "    top_pot_sent = [word for label in top_pot_sent for word in label]\n",
    "\n",
    "    label_score = matutils.cossim(tfidf[corpus[doc_id]], tfidf[dictionary.doc2bow(top_pot_sent)])\n",
    "    title = df['title'].loc[doc_id]\n",
    "    overview = df['overview'].loc[doc_id ]\n",
    "    topics = {}\n",
    "    for key, value in lda[corpus[doc_id]]:\n",
    "        topics.update({key : value})\n",
    "    \n",
    "    new_movie = Movie(doc_id, title, overview, topics, top_pot_labels, label_score )\n",
    "    return new_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dictionary of movie objects with theme lists and write to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = {}\n",
    "for doc_id in range(len(corpus)):\n",
    "    movies.update( {doc_id: get_movie_labels(doc_id)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('movie_labels.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['doc_id', 'title','overview', 'topics', 'labels','label_score' ]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    for movie in range(len(movies)):\n",
    "        try:\n",
    "            writer.writerow({'doc_id': movies[movie].doc_id, 'title':movies[movie].title, 'overview':movies[movie].overview ,\n",
    "                         'topics': movies[movie].topics, 'labels': movies[movie].labels , 'label_score': movies[movie].label_score})\n",
    "        except:\n",
    "            print(movie, 'failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dictionary of topics and their associated documents\n",
    "\n",
    "Gather the list of documents for each topic. The labels from the movie object is used instead of the full topic distribution. This will increase the accuracy of the filtered list of movies that match multiple topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "topics_docs = defaultdict(list)\n",
    "\n",
    "for i, doc in enumerate(movies):\n",
    "    for key in movies[i].labels:\n",
    "        topics_docs[key].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get global label based on frequency of words in associated documents\n",
    "\n",
    "\n",
    "\n",
    "1. Get documents associated with topic \n",
    "2. Build word frequency list for top x words\n",
    "3. Covert the phrase to a tfidf bow model\n",
    "4. Get cosine similarity for high potential phrases and top frequency list\n",
    "5. Keep the highest score as the top label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_label(topic_id):\n",
    "    num_words  = 10       #number of top frequency words to return\n",
    "    prob_thres = 0.1      #minimum threshold for probability of topic in document\n",
    "    topic_docs = []\n",
    "\n",
    "    for doc in topics_docs[topic_id]:\n",
    "        doc_topics = lda.get_document_topics(corpus[doc], minimum_probability=prob_thres)\n",
    "        for key, value in doc_topics:\n",
    "            if  key == topic_id:\n",
    "                topic_docs.append(corpus[doc])\n",
    "\n",
    "    freq_dict = {}\n",
    "    for doc in topic_docs:\n",
    "        for word, count in doc:\n",
    "            if word in freq_dict:\n",
    "                freq_dict[word] += count\n",
    "            else:\n",
    "                freq_dict.update({word: count})\n",
    "\n",
    "    sorted_dict =  sorted(freq_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "\n",
    "    freq_words = []\n",
    "    for key, value in sorted_dict[:num_words]:\n",
    "        freq_words.append(dictionary.id2token[key])\n",
    "    print('Word Frequency:',freq_words)\n",
    "    freq_corpus = tfidf[dictionary.doc2bow(freq_words)]\n",
    "\n",
    "    top_score = 0\n",
    "    top_score_match = []\n",
    "\n",
    "    for pot in top_matches[topic_id]:\n",
    "        pot_corpus = tfidf[dictionary.doc2bow(pot)]\n",
    "        #label score\n",
    "        pot_score = matutils.cossim(freq_corpus , pot_corpus)\n",
    "        if pot_score > top_score:\n",
    "            top_score_match = list(pot)\n",
    "            top_score = pot_score\n",
    "            \n",
    "    return top_score_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "============================\n",
    "\n",
    "\n",
    "# Demo Section\n",
    "\n",
    "============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "def print_movie_details(movie_id): \n",
    "    temp_movie = get_movie_labels(movie_id ) \n",
    "    print(\"Title:\", temp_movie.title)\n",
    "    print(\"Overview:\" ,temp_movie.overview)\n",
    "    pp.pprint(temp_movie.labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample document \n",
    "\n",
    "use: <br>\n",
    "print_movie_details(movie_id)<br>\n",
    "movie_id range from 0 - 45465\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_movie_details(1256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Browse topics\n",
    "\n",
    "topic_id ranges from 0 - 299 <br>\n",
    "get_top_label(topic_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browse_topic_id =  189\n",
    "print('LDA distribution:', lda.show_topic(browse_topic_id))\n",
    "print('Theme:',get_top_label(browse_topic_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find intersection of document lists for multiple topics to filter down to a small list movies\n",
    "\n",
    "topic_id ranges from 0 - 299 <br> \n",
    "chain set(topics_docs[topic_id]) together to find the intersection of documents for topics chained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id in list(set(topics_docs[45]) & set(topics_docs[189]) ):\n",
    "    print_movie_details(doc_id)\n",
    "    print(\"=======================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
